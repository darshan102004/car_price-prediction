import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import re

# Load dataset
df = pd.read_csv('cars24data.csv')

# Basic exploration
print("Dataset Shape:", df.shape)
print("Columns:", df.columns.tolist())
print("Missing Values:\n", df.isnull().sum())

# Clean Model Name by removing year prefixes
def clean_model_name(model_name):
    if isinstance(model_name, str):
        return re.sub(r'^\d{4}\s+', '', model_name).strip()
    return model_name

df['Model Name'] = df['Model Name'].apply(clean_model_name)
print("Unique Model Names after cleaning:\n", df['Model Name'].unique())

# Add Car_Age from Manufacturing_year
df['Car_Age'] = 2025 - df['Manufacturing_year']
df = df.drop(['Manufacturing_year'], axis=1)

# Define numerical and categorical columns
num_cols = ['Engine capacity', 'KM driven', 'Car_Age']
cat_cols = ['Model Name', 'Spare key', 'Transmission', 'Ownership', 'Fuel type', 'Imperfections', 'Repainted Parts']

# Verify columns exist in dataset
missing_cols = [col for col in num_cols + cat_cols if col not in df.columns]
if missing_cols:
    print(f"Warning: Columns {missing_cols} not found in dataset.")

# Fill missing values
for col in num_cols:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].median())

for col in cat_cols:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].mode()[0])

# Drop rows where target (Price) is missing
df = df.dropna(subset=['Price'])

# Encode categorical columns using OneHotEncoder
encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
cat_encoded = encoder.fit_transform(df[cat_cols])
cat_encoded_df = pd.DataFrame(cat_encoded, columns=encoder.get_feature_names_out(cat_cols))

# Concatenate encoded features
df = pd.concat([df.drop(cat_cols, axis=1), cat_encoded_df], axis=1)

# Split features and target
X = df.drop(['Price'], axis=1)
y = df['Price']

# Scale numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Train model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Set feature names for model
model.feature_names_in_ = X.columns.tolist()

# Predict and evaluate
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\nModel Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): ${mae:.2f}")
print(f"Mean Squared Error (MSE): ${mse:.2f}")
print(f"Root Mean Squared Error (RMSE): ${rmse:.2f}")
print(f"R-squared Score: {r2:.4f}")

# Save files for deployment
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

with open('encoder.pkl', 'wb') as f:
    pickle.dump(encoder, f)

with open('num_cols.pkl', 'wb') as f:
    pickle.dump(num_cols, f)

with open('cat_cols.pkl', 'wb') as f:
    pickle.dump(cat_cols, f)

print("Pickled files saved: model.pkl, scaler.pkl, encoder.pkl, num_cols.pkl, cat_cols.pkl")